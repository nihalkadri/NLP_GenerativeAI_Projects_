# NLP_GenerativeAI_Projects_

## Project 1: Practice_WordRepresentation_01.ipynb

### Description

The notebook `Practice_WordRepresentation_01.ipynb` is a personal project where I practiced essential NLP concepts and techniques to develop a strong, hands-on understanding of word representation and text processing. This notebook includes practical implementations of widely used NLP methods, offering insights into encoding, transforming, and analyzing text data effectively for downstream applications.

### Topics Covered

This project covers the following topics, organized as individual sections within the notebook:

1. **Word Representation Techniques**
    - **One-Hot Encoding**: Transforming words into one-hot encoded vectors.
    - **Bag of Words**: Converting text into word frequency vectors.
    - **N-grams**: Generating n-gram sequences to capture contextual information.
    - **TF-IDF (Term Frequency-Inverse Document Frequency)**: Applying weighting to terms based on their importance within and across documents.
    - **Word2Vec**: Learning vector representations of words through continuous bag-of-words (CBOW) and skip-gram models.
    - **GloVe Embeddings**: Pre-trained embeddings from the Global Vectors for Word Representation method.
    - **BERT (Bidirectional Encoder Representations from Transformers)**: Implementing contextualized embeddings using BERT, a transformer-based model.

2. **Word Operations**
    - **Tokenization**: Breaking down text into individual words or subword tokens.
    - **POS Tagging (Part of Speech Tagging)**: Labeling words with their grammatical roles.
    - **Stop Words Removal**: Identifying and removing common words (like 'the', 'is') that may not add significant meaning to text analysis.
    - **Named Entity Recognition (NER) Tagging**: Recognizing and categorizing proper nouns, such as names of people, places, and organizations.
    - **Stemming and Lemmatization**: Reducing words to their base or root forms.
	
## Project 2: Text_classification_with_Bert_tokenizers_02.ipynb

### Description

This project utilizes Hugging Faceâ€™s AutoTokenizer with the pre-trained checkpoint 'bert-base-uncased' to efficiently tokenize text data, enabling seamless integration with BERT model.



