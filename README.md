# NLP_GenerativeAI_Projects_

## Project 1: Practice_WordRepresentation_01.ipynb

### Description

The notebook `Practice_WordRepresentation_01.ipynb` is a personal project where I practiced essential NLP concepts and techniques to develop a strong, hands-on understanding of word representation and text processing. This notebook includes practical implementations of widely used NLP methods, offering insights into encoding, transforming, and analyzing text data effectively for downstream applications.

### Topics Covered

This project covers the following topics, organized as individual sections within the notebook:

1. **Word Representation Techniques**
    - **One-Hot Encoding**: Transforming words into one-hot encoded vectors.
    - **Bag of Words**: Converting text into word frequency vectors.
    - **N-grams**: Generating n-gram sequences to capture contextual information.
    - **TF-IDF (Term Frequency-Inverse Document Frequency)**: Applying weighting to terms based on their importance within and across documents.
    - **Word2Vec**: Learning vector representations of words through continuous bag-of-words (CBOW) and skip-gram models.
    - **GloVe Embeddings**: Pre-trained embeddings from the Global Vectors for Word Representation method.
    - **BERT (Bidirectional Encoder Representations from Transformers)**: Implementing contextualized embeddings using BERT, a transformer-based model.

2. **Word Operations**
    - **Tokenization**: Breaking down text into individual words or subword tokens.
    - **POS Tagging (Part of Speech Tagging)**: Labeling words with their grammatical roles.
    - **Stop Words Removal**: Identifying and removing common words (like 'the', 'is') that may not add significant meaning to text analysis.
    - **Named Entity Recognition (NER) Tagging**: Recognizing and categorizing proper nouns, such as names of people, places, and organizations.
    - **Stemming and Lemmatization**: Reducing words to their base or root forms.
	
## Project 2: Text_classification_with_Bert_tokenizers_02.ipynb

### Description

This project utilizes Hugging Faceâ€™s AutoTokenizer with the pre-trained checkpoint 'bert-base-uncased' to efficiently tokenize text data, enabling seamless integration with BERT model.

## Project 3: NLP_tasks_with_Transformers_03.ipynb

### Description

The notebook `NLP_tasks_with_Transformers_03.ipynb` leverages Hugging Face's Transformers library to explore various NLP tasks using different pre-trained models. This project demonstrates the versatility of transformer models in handling a range of applications, including classifying entire sentences, performing named entity recognition (NER), answering questions based on provided context, generating text summaries, filling in the blanks, and translating languages. Through practical implementations, the project showcases how to utilize the Transformers pipeline effectively for real-world NLP challenges.


