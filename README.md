# NLP_GenerativeAI_Projects_

## Project 1: Practice_WordRepresentation_01.ipynb

### Description

The notebook `Practice_WordRepresentation_01.ipynb` is a personal project where I practiced essential NLP concepts and techniques to develop a strong, hands-on understanding of word representation and text processing. This notebook includes practical implementations of widely used NLP methods, offering insights into encoding, transforming, and analyzing text data effectively for downstream applications.

### Topics Covered

This project covers the following topics, organized as individual sections within the notebook:

1. **Word Representation Techniques**
    - **One-Hot Encoding**: Transforming words into one-hot encoded vectors.
    - **Bag of Words**: Converting text into word frequency vectors.
    - **N-grams**: Generating n-gram sequences to capture contextual information.
    - **TF-IDF (Term Frequency-Inverse Document Frequency)**: Applying weighting to terms based on their importance within and across documents.
    - **Word2Vec**: Learning vector representations of words through continuous bag-of-words (CBOW) and skip-gram models.
    - **GloVe Embeddings**: Pre-trained embeddings from the Global Vectors for Word Representation method.
    - **BERT (Bidirectional Encoder Representations from Transformers)**: Implementing contextualized embeddings using BERT, a transformer-based model.

2. **Word Operations**
    - **Tokenization**: Breaking down text into individual words or subword tokens.
    - **POS Tagging (Part of Speech Tagging)**: Labeling words with their grammatical roles.
    - **Stop Words Removal**: Identifying and removing common words (like 'the', 'is') that may not add significant meaning to text analysis.
    - **Named Entity Recognition (NER) Tagging**: Recognizing and categorizing proper nouns, such as names of people, places, and organizations.
    - **Stemming and Lemmatization**: Reducing words to their base or root forms.
	
## Project 2: Text_classification_with_Bert_tokenizers_02.ipynb

### Description

This project utilizes Hugging Faceâ€™s AutoTokenizer with the pre-trained checkpoint 'bert-base-uncased' to efficiently tokenize text data, enabling seamless integration with BERT model.

## Project 3: NLP_tasks_with_Transformers_03.ipynb

### Description

The notebook `NLP_tasks_with_Transformers_03.ipynb` leverages Hugging Face's Transformers library to explore various NLP tasks using different pre-trained models. This project demonstrates the versatility of transformer models in handling a range of applications, including classifying entire sentences, performing named entity recognition (NER), answering questions based on provided context, generating text summaries, filling in the blanks, and translating languages. Through practical implementations, the project showcases how to utilize the Transformers pipeline effectively for real-world NLP challenges.

## Project 4: Sentiment_Analysis_with_Transformers_04.ipynb

### Description

The notebook `Sentiment_Analysis_with_Transformers_04.ipynb` focuses on performing sentiment analysis using Hugging Face's Transformers library. This project implements various pre-trained models, including DistilBERT, RoBERTa, and BERT, to classify the sentiment of text data. By leveraging these state-of-the-art transformer models, the project demonstrates how to effectively analyze and interpret sentiments in textual content, providing insights into the emotional tone conveyed in the data.

## Project 5: Query_Evaluation_for_Topic_Modelling_05.ipynb

### Description

The notebook `Query_Evaluation_for_Topic_Modelling_05.ipynb` explores topic modeling techniques using a dataset of queries related to various Microsoft products and services. The project involves preprocessing the input data, applying Latent Dirichlet Allocation (LDA) and Latent Semantic Analysis/Indexing (LSA/LSI) to identify underlying topics within the queries. Additionally, it utilizes pyLDAvis for topic modeling visualization, providing interactive insights into topic distributions. The model's effectiveness is further evaluated using the CoherenceModel from the `gensim` library, enabling a comprehensive assessment of topic quality and coherence.
 

